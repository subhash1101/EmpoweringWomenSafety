{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP-Je141HLXu",
        "outputId": "d2510a61-ea55-417f-a6aa-bbbeb0655129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install torch torchvision opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.spatial import distance"
      ],
      "metadata": {
        "id": "k_Jjmu9RJThv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the YOLOv5 model\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # 'yolov5s' is a small, pre-trained model\n",
        "\n",
        "# Load an image\n",
        "image_path = '/content/1 (6).jpg'\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Perform inference on the image\n",
        "results = model(image)\n",
        "\n",
        "# Get detected objects and filter for 'person'\n",
        "person_detections = results.xyxy[0].cpu().numpy()  # Bounding boxes in [x1, y1, x2, y2, confidence, class] format\n",
        "person_detections = [det for det in person_detections if det[5] == 0]  # Class 0 corresponds to 'person'\n",
        "\n",
        "# Extract bounding box centroids for persons\n",
        "centroids = []\n",
        "real_world_distances = []  # Define the list to hold the conversion factors\n",
        "\n",
        "# Assume real-world height of a person (e.g., average person height)\n",
        "REAL_WORLD_HEIGHT = 1.7  # in meters\n",
        "\n",
        "for det in person_detections:\n",
        "    x1, y1, x2, y2 = det[:4]\n",
        "    centroid_x = (x1 + x2) / 2\n",
        "    centroid_y = (y1 + y2) / 2\n",
        "    centroids.append((centroid_x, centroid_y))\n",
        "\n",
        "    # Calculate the pixel height of the person (height of bounding box)\n",
        "    pixel_height = y2 - y1\n",
        "\n",
        "    # Conversion factor (real-world height divided by pixel height)\n",
        "    conversion_factor = REAL_WORLD_HEIGHT / pixel_height  # meters per pixel\n",
        "    real_world_distances.append(conversion_factor)  # Store conversion factor for each person\n",
        "\n",
        "# Draw bounding boxes and centroids on the image\n",
        "for det in person_detections:\n",
        "    x1, y1, x2, y2, confidence, _ = det\n",
        "    cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "\n",
        "for (centroid_x, centroid_y) in centroids:\n",
        "    cv2.circle(image, (int(centroid_x), int(centroid_y)), 5, (0, 0, 255), -1)\n",
        "\n",
        "# Check if we have at least two persons detected\n",
        "if len(centroids) >= 2:\n",
        "    # Calculate the Euclidean distance between the first two persons in pixels\n",
        "    person1 = centroids[0]\n",
        "    person2 = centroids[1]\n",
        "    pixel_distance = distance.euclidean(person1, person2)\n",
        "\n",
        "    # Use the conversion factor (meters per pixel) from one of the detected persons\n",
        "    if real_world_distances:\n",
        "        # Assuming you use the first detected person's conversion factor\n",
        "        conversion_factor = real_world_distances[0]\n",
        "\n",
        "        # Convert pixel distance to real-world distance\n",
        "        real_world_distance = pixel_distance * conversion_factor\n",
        "        print(f\"Pixel distance: {pixel_distance} pixels\")\n",
        "        print(f\"Real-world distance: {real_world_distance:.2f} meters\")\n",
        "    else:\n",
        "        print(\"Conversion factor could not be determined.\")\n",
        "else:\n",
        "    print(\"Less than two persons detected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpkllHLKHV-p",
        "outputId": "2d63201c-185c-4418-c8c7-4f8ec56853fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2024-9-6 Python-3.10.12 torch-2.4.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n",
            "/root/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with amp.autocast(autocast):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pixel distance: 1025.8120857399017 pixels\n",
            "Real-world distance: 0.88 meters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Constants\n",
        "KNOWN_HEIGHT = 1.7  # Average person height in meters (as a reference)\n",
        "\n",
        "# Load YOLOv5 model for detecting people\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')\n",
        "\n",
        "# Load the image\n",
        "image_path = \"/content/2 (2).jpg\"  # Replace with your image path\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# YOLO object detection\n",
        "results = model(image)\n",
        "person_detections = results.xyxy[0].cpu().numpy()  # Bounding boxes in [x1, y1, x2, y2, confidence, class] format\n",
        "person_detections = [det for det in person_detections if det[5] == 0]  # Class 0 corresponds to 'person'\n",
        "\n",
        "# Assuming camera parameters\n",
        "focal_length = 800  # Assumed focal length in pixels (needs calibration)\n",
        "\n",
        "# Function to estimate real-world distance based on bounding box height\n",
        "def estimate_distance_from_height(bbox_height):\n",
        "    if bbox_height > 0:\n",
        "        # Estimating real-world distance using known height and focal length\n",
        "        distance = (KNOWN_HEIGHT * focal_length) / bbox_height\n",
        "        return distance\n",
        "    return None\n",
        "\n",
        "# Get centroids of each detected person\n",
        "centroids = []\n",
        "real_world_distances = []  # To store real-world distances\n",
        "\n",
        "# Process detected persons\n",
        "for det in person_detections:\n",
        "    x1, y1, x2, y2 = map(int, det[:4])\n",
        "\n",
        "    # Calculate the centroid of the bounding box\n",
        "    cx = (x1 + x2) // 2\n",
        "    cy = (y1 + y2) // 2\n",
        "\n",
        "    # Store the centroid\n",
        "    centroids.append((cx, cy))\n",
        "\n",
        "    # Calculate the height of the bounding box (y2 - y1 gives the height in pixels)\n",
        "    bbox_height = y2 - y1\n",
        "\n",
        "    # Estimate real-world distance based on bounding box height\n",
        "    real_distance = estimate_distance_from_height(bbox_height)\n",
        "    real_world_distances.append(real_distance)\n",
        "\n",
        "    # Print the real-world distance for each person\n",
        "    print(f\"Person at ({cx}, {cy}) has an estimated distance of {real_distance:.2f} meters\")\n",
        "\n",
        "    # Draw the bounding box and centroid on the image\n",
        "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    cv2.circle(image, (cx, cy), 5, (0, 0, 255), -1)\n",
        "\n",
        "# Calculate distances between each pair of persons\n",
        "num_persons = len(centroids)\n",
        "\n",
        "for i in range(num_persons):\n",
        "    for j in range(i + 1, num_persons):\n",
        "        # Calculate Euclidean distance in pixel space between centroids\n",
        "        pixel_dist = distance.euclidean(centroids[i], centroids[j])\n",
        "\n",
        "        # Use average height distance as a rough estimate for real-world distance\n",
        "        avg_height = (real_world_distances[i] + real_world_distances[j]) / 2\n",
        "\n",
        "        # Print the real-world distance between the two persons\n",
        "        print(f\"Distance between Person {i+1} and Person {j+1}: {avg_height:.2f} meters\")\n",
        "\n",
        "        # Draw the distance on the image\n",
        "        midpoint = ((centroids[i][0] + centroids[j][0]) // 2, (centroids[i][1] + centroids[j][1]) // 2)\n",
        "        cv2.line(image, centroids[i], centroids[j], (255, 0, 0), 2)\n",
        "        cv2.putText(image, f\"{avg_height:.2f}m\", midpoint, cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ULl1KYhEaDEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3bf18f1-ebc1-476f-9467-9bd889f07730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 ðŸš€ 2024-9-9 Python-3.10.12 torch-2.4.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IqpAFnTytST5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}