{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU3k7gAzPSLd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R-dVVfOQy4t",
        "outputId": "4a90af67-e01b-46f4-8d86-4eb4690e50d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1337 images belonging to 2 classes.\n",
            "Found 334 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "# Define paths to the dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/gender_dataset\"\n",
        "\n",
        "# Define parameters\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create data generators with data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2,  # Use 20% of the data for validation\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        ")\n",
        "\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',  # binary labels for two classes\n",
        "    subset='training',\n",
        ")\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lgPGKtXaQBz",
        "outputId": "02173a77-c82e-49a9-abbb-d9afe3c74370"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the pre-trained ResNet50 model, excluding the top layers\n",
        "base_model = ResNet50(input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "                      include_top=False,\n",
        "                      weights='imagenet')\n",
        "\n",
        "# Freeze most of the base model's layers, leaving the last few layers trainable\n",
        "for layer in base_model.layers[:-10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),  # L2 regularization\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model with a low learning rate for fine-tuning\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "# model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmDY1Y33aZGL",
        "outputId": "49aad597-418f-4dde-efe0-d38c578a9d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m394s\u001b[0m 8s/step - accuracy: 0.4699 - loss: 1.2028 - val_accuracy: 0.5031 - val_loss: 1.1704 - learning_rate: 1.0000e-05\n",
            "Epoch 2/20\n",
            "\u001b[1m 1/41\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 109ms/step - accuracy: 0.4688 - loss: 1.2506"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 140ms/step - accuracy: 0.4688 - loss: 1.2506 - val_accuracy: 0.4286 - val_loss: 1.1981 - learning_rate: 1.0000e-05\n",
            "Epoch 3/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m146s\u001b[0m 2s/step - accuracy: 0.5269 - loss: 1.1634 - val_accuracy: 0.4906 - val_loss: 1.1635 - learning_rate: 1.0000e-05\n",
            "Epoch 4/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5938 - loss: 1.1206 - val_accuracy: 0.7143 - val_loss: 1.0793 - learning_rate: 1.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 2s/step - accuracy: 0.5674 - loss: 1.1262 - val_accuracy: 0.5063 - val_loss: 1.1460 - learning_rate: 1.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5938 - loss: 1.0923 - val_accuracy: 0.3571 - val_loss: 1.1946 - learning_rate: 1.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.5743 - loss: 1.1238 - val_accuracy: 0.5063 - val_loss: 1.1523 - learning_rate: 1.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5312 - loss: 1.1074 - val_accuracy: 0.5000 - val_loss: 1.1673 - learning_rate: 5.0000e-06\n",
            "Epoch 9/20\n",
            "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 2s/step - accuracy: 0.5903 - loss: 1.1126 - val_accuracy: 0.5219 - val_loss: 1.1206 - learning_rate: 5.0000e-06\n"
          ]
        }
      ],
      "source": [
        "# Define the number of epochs\n",
        "EPOCHS = 20\n",
        "\n",
        "# Callbacks for learning rate reduction and early stopping\n",
        "lr_schedule = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[lr_schedule, early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKrVH3zPbpTm"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the validation set\n",
        "val_loss, val_acc = model.evaluate(validation_generator)\n",
        "print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqomFXCgheIC"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess an image for prediction\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "def predict_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalize\n",
        "\n",
        "    # Predict the class\n",
        "    prediction = model.predict(img_array)\n",
        "    if prediction < 0.5:\n",
        "        print(\"Prediction: Man\")\n",
        "    else:\n",
        "        print(\"Prediction: Woman\")\n",
        "\n",
        "# Test with a sample image\n",
        "predict_image(\"/content/maa.jpg\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}